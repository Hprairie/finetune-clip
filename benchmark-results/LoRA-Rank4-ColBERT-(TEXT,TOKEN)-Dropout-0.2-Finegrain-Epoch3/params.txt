batchsize: 256
dataset: /work/09753/hprairie/ls6/.datasets/coco/images/val2017
dataset_ann: /work/09753/hprairie/ls6/.datasets/coco/annotations/captions_val2017.json
finetune_args: Namespace(train_data='/work/09753/hprairie/ls6/.datasets/coco/annotations/captions_train2017.csv', train_data_upsampling_factors=None, val_data=None, train_num_samples=None, val_num_samples=None, dataset_type='csv', dataset_resampled=False, csv_separator='\t', csv_img_key='filepath', csv_caption_key='title', imagenet_val=None, imagenet_v2=None, logs='./logs/', log_local=False, name='ViT-B-32-LoRA-Epoch3-Rank4-ColBERT-(TEXT,TOKEN)-Drop0.2', workers=2, batch_size=128, epochs=3, epochs_cooldown=None, lr=1e-05, beta1=0.9, beta2=0.98, eps=1e-06, wd=0.1, warmup=1000, use_bn_sync=False, skip_scheduler=False, lr_scheduler='cosine', lr_cooldown_end=0.0, lr_cooldown_power=1.0, save_frequency=1, save_most_recent=False, zeroshot_frequency=2, val_frequency=1, resume=None, precision='amp', model='ViT-B-32', pretrained='openai', pretrained_image=False, lock_image=False, lock_image_unlocked_groups=0, lock_image_freeze_bn_stats=False, image_mean=None, image_std=None, image_interpolation=None, image_resize_mode=None, aug_cfg={}, grad_checkpointing=False, local_loss=False, gather_with_grad=False, force_image_size=None, force_quick_gelu=False, force_patch_dropout=None, force_custom_text=False, torchscript=False, torchcompile=False, trace=False, accum_freq=1, dist_url='env://', dist_backend='nccl', report_to='wandb', wandb_notes='', wandb_project_name='ViT-B-32-Finetune', debug=False, copy_codebase=False, horovod=False, ddp_static_graph=False, no_set_device_rank=False, seed=0, grad_clip_norm=None, lock_text=False, lock_text_unlocked_layers=0, lock_text_freeze_layer_norm=False, log_every_n_steps=100, coca_caption_loss_weight=2.0, coca_contrastive_loss_weight=1.0, remote_sync=None, remote_sync_frequency=300, remote_sync_protocol='s3', delete_previous_checkpoint=False, use_bnb_linear=None, siglip=False, colbert=True, colbert_dropout=0.2, colbert_local_contrastive='token-wise', colbert_global_contrastive='text-wise', sparc=False, sparc_global_lambda=1.0, sparc_local_lambda=1.0, lora='4:1', freeze_layers=None, unfreeze_norm=False, linear_probing=None)
finetune_path: logs/ViT-B-32-LoRA-Epoch3-Rank4-ColBERT-(TEXT,TOKEN)-Drop0.2
k: [1, 5, 10, 50]
model: None
name: LoRA-Rank4-ColBERT-(TEXT,TOKEN)-Dropout-0.2-Finegrain-Epoch3
pretrained: logs/ViT-B-32-LoRA-Epoch3-Rank4-ColBERT-(TEXT,TOKEN)-Drop0.2/checkpoints/epoch_3.pt
reg_retrieval: False
